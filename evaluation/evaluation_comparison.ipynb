{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "evaluations_dir_path = Path(\"/home/watson_chua/efs/hansard_finetuning/data/evaluations\")\n",
    "models_to_compare = [\"gpt4_zero_shot\", \"gpt4_one_shot\", \"llama3\", \"gemma2\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gpt4_zero_shot', 'llama3'),\n",
       " ('gpt4_zero_shot', 'gemma2'),\n",
       " ('gpt4_one_shot', 'llama3'),\n",
       " ('gpt4_one_shot', 'gemma2'),\n",
       " ('llama3', 'gemma2')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_combination = []\n",
    "for i in range(len(models_to_compare)):\n",
    "    for j in range(i+1, len(models_to_compare)):\n",
    "        if models_to_compare[i].startswith(\"gpt4\") and models_to_compare[j].startswith(\"gpt4\"):\n",
    "            continue\n",
    "        pairwise_combination.append((models_to_compare[i], models_to_compare[j]))\n",
    "pairwise_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Evaluation String to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from llm_utils.post_processing import clean_json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_evaluation_to_df(input_path):\n",
    "    with input_path.open('r') as f:\n",
    "        lines = f.readlines()\n",
    "    json_lines =  [json.loads(l) for l in lines]\n",
    "    df_eval = pd.DataFrame([clean_json_output(l[\"evaluation\"]) for l in json_lines])\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Preference Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_zero_shot vs. llama3\n",
      "winner\n",
      "b    212\n",
      "a     92\n",
      "Name: count, dtype: int64\n",
      "gpt4_zero_shot vs. gemma2\n",
      "winner\n",
      "b    221\n",
      "a     83\n",
      "Name: count, dtype: int64\n",
      "gpt4_one_shot vs. llama3\n",
      "winner\n",
      "b    192\n",
      "a    112\n",
      "Name: count, dtype: int64\n",
      "gpt4_one_shot vs. gemma2\n",
      "winner\n",
      "b    205\n",
      "a     99\n",
      "Name: count, dtype: int64\n",
      "llama3 vs. gemma2\n",
      "winner\n",
      "b    188\n",
      "a    116\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for model_1, model_2 in pairwise_combination:\n",
    "    df_eval = convert_evaluation_to_df(evaluations_dir_path/Path(f\"{model_1}_{model_2}_evaluation.jsonl\"))\n",
    "    print(f\"{model_1} vs. {model_2}\")\n",
    "    print(df_eval[\"winner\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root_dir = \"/home/watson_chua/efs/axolotl/data/evaluations/hansard/\"\n",
    "def compare_ragas(subdir, llm1):\n",
    "    df_ragas = pd.read_csv(output_root_dir + subdir + \"/\" + \"{llm1}_ragas_claude.csv\".format(llm1=llm1))\n",
    "    print(df_ragas.dropna(how=\"any\",axis=0,subset=[\"faithfulness\", \"answer_relevancy\", \"answer_correctness\"]).mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_zero_shot\n",
      "faithfulness          0.855116\n",
      "answer_relevancy      0.662348\n",
      "answer_similarity     0.785876\n",
      "answer_correctness    0.653498\n",
      "dtype: float64\n",
      "gpt4_one_shot\n",
      "faithfulness          0.840792\n",
      "answer_relevancy      0.669053\n",
      "answer_similarity     0.784777\n",
      "answer_correctness    0.663694\n",
      "dtype: float64\n",
      "llama3\n",
      "faithfulness          0.862831\n",
      "answer_relevancy      0.616783\n",
      "answer_similarity     0.807248\n",
      "answer_correctness    0.783219\n",
      "dtype: float64\n",
      "gemma2\n",
      "faithfulness          0.860389\n",
      "answer_relevancy      0.609027\n",
      "answer_similarity     0.801928\n",
      "answer_correctness    0.782969\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for model_1 in models_to_compare:\n",
    "    df_ragas = pd.read_csv(evaluations_dir_path / Path(f\"{model_1}_ragas.csv\"))\n",
    "    print(model_1)\n",
    "    print(df_ragas.dropna(how=\"any\",axis=0,subset=[\"faithfulness\", \"answer_relevancy\", \"answer_similarity\", \"answer_correctness\"]).mean(numeric_only=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
