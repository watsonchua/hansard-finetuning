{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb08f6ad-54f5-4f15-9c3c-2937ccd361e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87cba45-8b1b-478a-bd74-639275421e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "from ftfy import fix_text\n",
    "import os\n",
    "import json\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0301\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hansard_to_docs(hansard_filepaths, max_chunk_length=800):\n",
    "    counter = 1\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(encoding_name='cl100k_base',\n",
    "            chunk_size=max_chunk_length,                                                         \n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "        \n",
    "    doc_chunks = []\n",
    "\n",
    "    for hansard_fp in tqdm(hansard_filepaths):\n",
    "        filename_truncated = hansard_fp[:-6].split(os.path.sep)[-1]\n",
    "        with open(hansard_fp, 'r') as f:\n",
    "            date = hansard_fp[-16:-6]\n",
    "            lines = f.readlines()\n",
    "            temp_chunks = []\n",
    "            chunk_length = 0\n",
    "            for l in lines:\n",
    "                hansard_info = json.loads(l)\n",
    "                speaker = hansard_info['speaker']\n",
    "                text = hansard_info['text']\n",
    "                \n",
    "                content = date + ':\\t' + speaker + ':\\t' + text\n",
    "                content_length = len(enc.encode(content))\n",
    "                \n",
    "                # split large chunks\n",
    "                if content_length > max_chunk_length:\n",
    "                    splits = text_splitter.split_text(content)\n",
    "                    \n",
    "                    for split_no, s in enumerate(splits):\n",
    "                        source_key = 'C' + \"{:05d}\".format(counter)\n",
    "                        if split_no > 0:\n",
    "                            s = date + ':\\t' + speaker + ':\\t' + s\n",
    "                        split_length = len(enc.encode(s))\n",
    "                        doc = Document(page_content=s, metadata={\"filename\": filename_truncated, \"length\": split_length, \"clause_no\": '', \"split_no\": str(split_no), \"source\": source_key})\n",
    "                        doc_chunks.append(doc)\n",
    "                        \n",
    "                        # print('[add chunk split]:', s)\n",
    "\n",
    "                        \n",
    "                        counter +=1\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    # add to chunks until max length is reached\n",
    "                    if content_length + chunk_length <= max_chunk_length:\n",
    "                        temp_chunks.append(hansard_info)\n",
    "                        chunk_length += content_length\n",
    "                        \n",
    "                    else:\n",
    "                        # write as document if chunk size exceeded\n",
    "                        if temp_chunks:\n",
    "                            source_key = 'C' + \"{:05d}\".format(counter)\n",
    "                            merged_chunks_text = '\\n\\n'.join([date + ': ' + t['speaker'] + ': ' + t['text'] for t in temp_chunks])\n",
    "                            split_length = len(enc.encode(merged_chunks_text))\n",
    "                            doc = Document(page_content=merged_chunks_text, metadata={\"filename\": filename_truncated, \"length\": split_length, \"clause_no\": '', \"split_no\": 0, \"source\": source_key})\n",
    "                            doc_chunks.append(doc)\n",
    "                            \n",
    "                            \n",
    "                            # print('[add chunk join]:', merged_chunks_text)\n",
    "\n",
    "                            \n",
    "                            temp_chunks = []\n",
    "                            chunk_length = 0\n",
    "                            counter +=1\n",
    "                            \n",
    "                \n",
    "            # write out residual chunks\n",
    "            if temp_chunks:\n",
    "                source_key = 'C' + \"{:05d}\".format(counter)\n",
    "                merged_chunks_text = '\\n\\n'.join([date + ': ' + t['speaker'] + ': ' + t['text'] for t in temp_chunks])\n",
    "                split_length = len(enc.encode(merged_chunks_text))\n",
    "                doc = Document(page_content=merged_chunks_text, metadata={\"filename\": filename_truncated, \"length\": split_length, \"clause_no\": '', \"split_no\": 0, \"source\": source_key})\n",
    "                doc_chunks.append(doc)\n",
    "\n",
    "                # print('[add chunk residual]:', merged_chunks_text)\n",
    "\n",
    "                                                                                        \n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db861738-9f74-4165-93a4-14d38904b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_fps_2020s = glob('/home/watsonchua/work/others/hansard_extract_parse/sessions_by_sitting/sitting_202*.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c267604-b536-4f46-b21e-2b53b6ad7651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ebde60c6e2499293a8b4086addce74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hansard_chunks = hansard_to_docs(hansard_fps_2020s, max_chunk_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0683a5d-0cb4-4094-84d3-fa5cbfbc02fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[599, 599, 598, 598, 598, 597, 597, 597, 597, 597]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([l.metadata['length'] for l in hansard_chunks], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e5d1a31-aed8-4df2-90fc-84b50023dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import toml, openai\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "secrets = toml.load('/home/watsonchua/work/im_question_answering/.streamlit/secrets.toml')\n",
    "\n",
    "openai_api_key = secrets['openai_api_key_azure']\n",
    "openai.api_key = openai_api_key\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://govtext-ds-experiment.openai.azure.com/\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "# azure_completion_engine = \"text-davinci-003-pretrained\"\n",
    "azure_completion_engine = \"gpt-35-turbo\"\n",
    "azure_embedding_engine = \"text-embedding-ada-002\"\n",
    "\n",
    "oai_embedder = OpenAIEmbeddings(query_model_name=azure_embedding_engine, document_model_name=azure_embedding_engine, openai_api_key=openai_api_key, chunk_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5355409-c234-42b4-abce-b3c98117d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(hansard_chunks, oai_embedder)\n",
    "db.save_local('./hansard_docs_2020s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7f619-d883-4a61-949f-65f115ba45ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
